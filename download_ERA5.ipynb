{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading ERA5 Data\n",
    "\n",
    "ERA5 data is downloaded using the NetCDF download protocols. Some variables may only be available in GRIB format - however, all standard climatological variables will be available using this script. ERA5 files are available in hourly timesteps; this code allows for resampling the data at lower frequencies (see the section on [Subsetting and Resampling](#subset)).\n",
    "\n",
    "The ERA5 API must be installed first; follow this [Guide](https://cds.climate.copernicus.eu/api-how-to) for more information. (the easiest way is to use `pip` to install `cdsapi`).\n",
    "\n",
    "ERA5 files are 0.25$^\\circ$ hourly data - this results in very large files. This code is built on the assumption that for stability of the download and efficiency of memory use, data is downloaded one temporal chunk at a time (by default one year at a time). For reference, a 20 x 60 degree box by the equator of one variable takes around 10 minutes per year to prepare and download. Each temporal chunk is downloaded separately and saved in a temporary file (these files are useful for checking if the download is working correctly, and to start working with data) - once all the temporary files are downloaded, they are concatenated into one single file for the whole temporal range, and the temporary files are deleted. Set a bounding box for the download in `geographic_vars`.\n",
    "\n",
    "Currently, only multiples of full years at a time can be downloaded; changing this is a relatively easy fix in the `c.retrieve()` call if it is needed.\n",
    "\n",
    "The code checks for previously downloaded and processed files (based on the output temporal resolution and subset) and ignores them with a message (this is the case for both temporary and concatenated files - but previously downloaded temporary files are of course included in the concatenated file either way).\n",
    "\n",
    "Files are placed in a subdirectory of the `raw_data_dir` set in the `dwnld_config.py` file `[raw_data_dir]/ERA5/`. If this subdirectory doesn't yet exist, it is created. Output files have the following filename: \n",
    "\n",
    "`[raw_data_dir]/ERA5/[short_name]_[output_freq_name]_ERA5_historical_reanalysis_[allyears[0]0101-allyears[-1]1231]_[fn_suffix].nc` \n",
    "\n",
    "with `[raw_data_dir]` set from the `config` file, `[short_name]` set in the `download_vars`, `output_freq_name` set in the `resampling_vars` (unless no time resampling is conducted; in this case `_hr_` is used), and `fn_suffix` set in the `geographic_vars`. \n",
    "\n",
    "**Summary of changes to downloaded ERA5 file**\n",
    "- filename changed to CMIP5 standard, from the procedurally-generated unintelligible ERA5 file\n",
    "- \"latitude\" changed to \"lat\" and \"longitude\" changed to \"lon\"\n",
    "- variable short named changed to `download_vars['short_name']`\n",
    "- data is resampled temporally (optional) from hourly to anything allowed by the `pandas` resampling syntax\n",
    "\n",
    "**Useful links** \n",
    "Copernicus Data Store (where ERA5 data is located) data download page: https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=form\n",
    "\n",
    "ERA5 on the UCAR Climate Data Guide: https://climatedataguide.ucar.edu/climate-data/era5-atmospheric-reanalysis\n",
    "\n",
    "ERA5 documentation: https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get config file \n",
    "import dwnld_config as cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "Set which variables to download. \n",
    "- `short_name`: variable shorthand (e.g. `tas`) used as filename identifier (e.g. `tas_*.nc`) in the output filename and as the variable name in the file itself\n",
    "- `long_name`: ERA5 variable identifier used to find file to download\n",
    "\n",
    "The `long_name` for each variable can be found in the [ERA5 Parameter Database](https://apps.ecmwf.int/codes/grib/param-db) - for files available in NetCDF (filter using the NetCDF button on the left), the name will be listed as \"cfName\" in the table at the bottom of the page for each relevant variable, under the NetCDF tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set which variables to download\n",
    "download_vars = [{'short_name':'vwt','long_name':'vertical_integral_of_northward_water_vapour_flux'}, \n",
    "                  {'short_name':'uwt','long_name':'vertical_integral_of_eastward_water_vapour_flux'},\n",
    "                  {'short_name':'u100','long_name':'100m_u_component_of_wind'},\n",
    "                  {'short_name':'v100','long_name':'100m_v_component_of_wind'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"subset\"></a>Subsetting and Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set geographic extent of download\n",
    "geographic_vars = {'bbox':[13,33,-7,59], # Max lat, min lon, min lat, max lon\n",
    "                   'fn_suffix':'_WIndOcean'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set years to process\n",
    "all_years = np.arange(1979,2015)\n",
    "\n",
    "# How many years to be processed at a time (may impact performance; detailed checks forthcoming)\n",
    "chunk_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERA5 data is released as hourly data. This code supports resampling using the `xarray.Dataset.resample()` function ([docs here](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.resample.html)), which is based on the `pandas.resample()` functionality. The next cell adjusts whether data is resampled. \n",
    "\n",
    "The `resample_level` value is piped into the `freq` call of the `.resample` indexer. Useful, common options include: \n",
    "- `D`: daily\n",
    "- `M`: month (end)\n",
    "- `MS`: month (beginning)\n",
    "\n",
    "A full list of options is available in the `pandas` documentation [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set resampling\n",
    "resampling_vars = {'resamp':True, # whether to resample from hourly (if false, the rest is ignored)\n",
    "                   'resample_level':'D', # what level to resample to \n",
    "                   'output_freq_name':'day'} # what the frequency should be called in the filename, attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(cfg.lpaths['raw_data_dir']):\n",
    "    os.mkdir(cfg.lpaths['raw_data_dir'])\n",
    "    print(cfg.lpaths['raw_data_dir']+' created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break up years into chunks\n",
    "chunks = [all_years[i:i + chunk_size] for i in range(0, len(all_years), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cdsapi.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for download_var in download_vars:\n",
    "    \n",
    "    if resampling_vars['resamp']:\n",
    "        fn_final = (cfg.lpaths['raw_data_dir']+'ERA5/'+download_var['short_name']+\n",
    "                    '_'+resampling_vars['output_freq_name']+\n",
    "                    '_ERA5_historical_reanalysis_'+\n",
    "                    str(all_years[0])+'0101-'+str(all_years[-1])+'1231'+\n",
    "                    geographic_vars['fn_suffix']+'.nc')\n",
    "    else:\n",
    "        fn_final = (cfg.lpaths['raw_data_dir']+'ERA5/'+download_var['short_name']+\n",
    "                    '_hr_ERA5_historical_reanalysis_'+\n",
    "                    str(all_years[0])+'0101-'+str(all_years[-1])+'1231'+\n",
    "                    geographic_vars['fn_suffix']+'.nc')\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(fn_final):\n",
    "\n",
    "        for chunk in chunks:\n",
    "            # Set filename for temporary file\n",
    "            fn0 = (cfg.lpaths['raw_data_dir']+'ERA5/'+download_var['short_name']+\n",
    "                   '_hr_ERA5_historical_reanalysis_'+str(chunk[0])+'0101-'+str(chunk[-1])+'1231'+geographic_vars['fn_suffix']+'.nc')\n",
    "            \n",
    "            # If the file does not yet exist, download it\n",
    "            if not os.path.exists(re.sub('hr','day',fn0)): \n",
    "\n",
    "                c.retrieve(\n",
    "                    'reanalysis-era5-single-levels',\n",
    "                    {\n",
    "                        'product_type': 'reanalysis',\n",
    "                        'format': 'netcdf',\n",
    "                        'variable': download_var['long_name'],\n",
    "                        'year': [str(ch) for ch in chunk],\n",
    "                        'month': [\n",
    "                            '01', '02', '03',\n",
    "                            '04', '05', '06',\n",
    "                            '07', '08', '09',\n",
    "                            '10', '11', '12',\n",
    "                        ],\n",
    "                        'day': [\n",
    "                            '01', '02', '03',\n",
    "                            '04', '05', '06',\n",
    "                            '07', '08', '09',\n",
    "                            '10', '11', '12',\n",
    "                            '13', '14', '15',\n",
    "                            '16', '17', '18',\n",
    "                            '19', '20', '21',\n",
    "                            '22', '23', '24',\n",
    "                            '25', '26', '27',\n",
    "                            '28', '29', '30',\n",
    "                            '31',\n",
    "                        ],\n",
    "                        'time': [\n",
    "                            '00:00', '01:00', '02:00',\n",
    "                            '03:00', '04:00', '05:00',\n",
    "                            '06:00', '07:00', '08:00',\n",
    "                            '09:00', '10:00', '11:00',\n",
    "                            '12:00', '13:00', '14:00',\n",
    "                            '15:00', '16:00', '17:00',\n",
    "                            '18:00', '19:00', '20:00',\n",
    "                            '21:00', '22:00', '23:00',\n",
    "                        ],\n",
    "                        'area': geographic_vars['bbox'],\n",
    "                    },\n",
    "                    fn0)\n",
    "\n",
    "\n",
    "                tmp = xr.open_dataset(fn0)\n",
    "\n",
    "                # Rename variables\n",
    "                tmp = tmp.rename({'latitude':'lat','longitude':'lon'})\n",
    "\n",
    "                if download_var['short_name'] not in tmp.var().variables.keys():\n",
    "                    tmp = tmp.rename({[k for k in tmp.var().variables.keys()][0]:download_var['short_name']})\n",
    "\n",
    "                if resampling_vars['resamp']:\n",
    "                    # Resample temporally\n",
    "                    tmp = tmp.resample(time=resampling_vars['resample_level']).mean()\n",
    "                    fn1 = re.sub('hr',resampling_vars['output_freq_name'],fn0)\n",
    "                    \n",
    "                    # Export\n",
    "                    tmp.to_netcdf(fn1)\n",
    "\n",
    "                    # Remove old file\n",
    "                    os.remove(fn0)\n",
    "                else:\n",
    "                    fn1 = fn0\n",
    "\n",
    "                    # Remove old file (reverse order from the resampling case \n",
    "                    # above, which gives a new filename, because to_netcdf \n",
    "                    # doesn't overwrite)\n",
    "                    os.remove(fn0)\n",
    "                    \n",
    "                    # Export\n",
    "                    tmp.to_netcdf(fn1)\n",
    "\n",
    "\n",
    "                print(fn1+' processed!')\n",
    "            else: \n",
    "                print(fn1+' already exists; skipped.')\n",
    "\n",
    "        # Open all the files (by wildcarding the date filename segment)\n",
    "        ds = xr.open_mfdataset(cfg.lpaths['raw_data_dir']+'ERA5/'+download_var['short_name']+\n",
    "                                '_'+resampling_vars['output_freq_name']+\n",
    "                                '_ERA5_historical_reanalysis_*'+geographic_vars['fn_suffix']+'.nc',\n",
    "                               combine='by_coords')\n",
    "\n",
    "        # Save the concatenated file across all years \n",
    "        ds.to_netcdf(fn_final)\n",
    "\n",
    "        # Remove the component files \n",
    "        for chunk in chunks:\n",
    "            os.remove(cfg.lpaths['raw_data_dir']+'ERA5/'+download_var['short_name']+\n",
    "                       '_'+resampling_vars['output_freq_name']+\n",
    "                       '_ERA5_historical_reanalysis_'+\n",
    "                       str(chunk[0])+'0101-'+str(chunk[-1])+'1231'+\n",
    "                       geographic_vars['fn_suffix']+'.nc')\n",
    "    else:\n",
    "        print(fn_final+' already exists, skipped.')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
